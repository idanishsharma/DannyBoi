{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most of the code is for Step by Step Understanding\n",
    "\n",
    "\n",
    "# Process is :\n",
    "### Loading Model and Storing Lables in a Variable\n",
    "### loading the Image and Creating the Blob\n",
    "### Extracting the co-ordinates of the requred Bounded Boxes and their classes\n",
    "### Deleting the duplicate Rectangles and finally plotting Rectangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNet('Z:/SimpliLearn/Advanced Deep Learning and Computer Vision/YOLO Data/YOLO Coco Data/Dependencies/yolo-object-detection/yolo-coco/yolov3.weights','Z:/SimpliLearn/Advanced Deep Learning and Computer Vision/YOLO Data/YOLO Coco Data/Dependencies/yolo-object-detection/yolo-coco/yolov3.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n"
     ]
    }
   ],
   "source": [
    "classes = []\n",
    "with open('Z:/SimpliLearn/Advanced Deep Learning and Computer Vision/YOLO Data/YOLO Coco Data/Dependencies/yolo-object-detection/yolo-coco/coco.names','r') as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "print(classes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = net.getLayerNames() #we're getting all layer names\n",
    "outputlayers = net.getUnconnectedOutLayersNames()  #we're getting output layers here. hese layers will be used to get the output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function destroyAllWindows>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread('Z:/SimpliLearn/Advanced Deep Learning and Computer Vision/YOLO Data/YOLO Coco Data/Dependencies/yolo-object-detection/images/living_room.jpg')\n",
    "#Detecting Objects in the Image before giving it to Algorithm.\n",
    "#We'll convert this image into a blob. A blob is a method to extract features from an image\n",
    "#Let's Detect \n",
    "height, width, channels = img.shape\n",
    "blob = cv2.dnn.blobFromImage(img, 0.00392, (416,416),(0,0,0), True, crop=False)#Ture means we're coverting to RGB ie, we will change the channel here\n",
    "#Displaying Blobs\n",
    "for b in blob:\n",
    "    for n,img_blob in enumerate(b):\n",
    "        cv2.imshow(str(n),img_blob) \n",
    "cv2.imshow('Image',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.setInput(blob)\n",
    "outs = net.forward(outputlayers) #we're saying that we want to use output layers to get final result\n",
    "#Now we have all the required information to extract the results and this info is stored in the variable outs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for each detetion from each output layer \n",
    "#### get the confidence, class id, bounding box params\n",
    "#### and ignore weak detections (confidence < 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing Info on Screen\n",
    "for out in outs: #outs is basically outputs of all 3 output layers and out is each output layer. \n",
    "    for detection in out: #detection is each bounding box in each layer\n",
    "        scores = detection[5:] #Evet bounding box has 5 values i.e, (x center, y center, width, height, confidence) and probability for each class\n",
    "        class_id = np.argmax(scores) #the index of max score for a class ID is extracted\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > 0.5: #confidence can go from 0-1\n",
    "            #Object is Detected\n",
    "            # Now we're getting Co-ordinates of the object Detected\n",
    "            center_x = int(detection[0] * width) #x coordinate of center\n",
    "            center_y = int(detection[1] * height) #y coordinate of center\n",
    "            w = int(detection[2] * width) #width\n",
    "            h = int(detection[3] * height) #height\n",
    "            \n",
    "            #Let's plot center of Center of required Bounded Boxes \n",
    "            #cv2.circle(img, (center_x, center_y), 10, (0,255,0),2) #(0,255,0) means Green color\n",
    "            \n",
    "cv2.imshow(\"Image\",img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rectangle Co-ordinates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same code again but with rectangle created properly\n",
    "#Showing Info on Screen\n",
    "for out in outs: #outs is basically outputs of all 3 output layers and out is each output layer. \n",
    "    for detection in out: #detection is each bounding box in each layer\n",
    "        scores = detection[5:] #Evet bounding box has 5 values i.e, (x center, y center, width, height, confidence) and probability for each class\n",
    "        class_id = np.argmax(scores) #the index of max score for a class ID is extracted\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > 0.5: #confidence can go from 0-1\n",
    "            #Object is Detected\n",
    "            # Now we're getting Co-ordinates of the object Detected\n",
    "            center_x = int(detection[0] * width) #x coordinate of center\n",
    "            center_y = int(detection[1] * height) #y coordinate of center\n",
    "            w = int(detection[2] * width) #width\n",
    "            h = int(detection[3] * height) #height\n",
    "            \n",
    "            #Rectangle Co-ordinates\n",
    "            x = int(center_x - w / 2) #Top left of x\n",
    "            y = int(center_y - h /2) #Top Left of Y\n",
    "            \n",
    "            cv2.rectangle(img,(x,y),(x + w,y + h), (0,255,0),1)\n",
    "            \n",
    "            \n",
    "cv2.imshow(\"Image\",img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Results aren't very accurate yet but good start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "# Let's show Info on Screen about the Lables\n",
    "boxes = []\n",
    "confidences = []\n",
    "class_ids = []\n",
    "\n",
    "#Same code again but with rectangle created properly\n",
    "#Showing Info on Screen\n",
    "for out in outs: #outs is basically outputs of all 3 output layers and out is each output layer. \n",
    "    for detection in out: #detection is each bounding box in each layer\n",
    "        scores = detection[5:] #Evet bounding box has 5 values i.e, (x center, y center, width, height, confidence) and probability for each class\n",
    "        class_id = np.argmax(scores) #the index of max score for a class ID is extracted\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > 0.5: #confidence can go from 0-1\n",
    "            #Object is Detected\n",
    "            # Now we're getting Co-ordinates of the object Detected\n",
    "            center_x = int(detection[0] * width) #x coordinate of center\n",
    "            center_y = int(detection[1] * height) #y coordinate of center\n",
    "            w = int(detection[2] * width) #width\n",
    "            h = int(detection[3] * height) #height\n",
    "            \n",
    "            #Rectangle Co-ordinates\n",
    "            x = int(center_x - w / 2) #Top left of x\n",
    "            y = int(center_y - h /2) #Top Left of Y\n",
    "            \n",
    "        \n",
    "            boxes.append([x,y,w,h])\n",
    "            confidences.append(float(confidence))\n",
    "            class_ids.append(class_id)\n",
    "            \n",
    "print(len(boxes)) # 12 objects deteced \n",
    "\n",
    "number_objects_detected = len(boxes)\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "for i in range(len(boxes)):\n",
    "    x,y,w,h = boxes[i]\n",
    "    label  = str(classes[class_ids[i]])\n",
    "    cv2.rectangle(img, (x,y),(x + w, y + h), (0,255,0),2)\n",
    "    cv2.putText(img,label, (x,y + 30), font, 1, (255,255,255), 1)\n",
    "cv2.imshow(\"Image\",img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As we see there are multiple Rectangles around the same object\n",
    "    i,e. one object is being identified by 2 boxes. Add this following code to remove multiple boxes. This fn. uses a threshold value to remove boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1]\n",
      " [ 0]\n",
      " [ 5]\n",
      " [ 7]\n",
      " [11]]\n"
     ]
    }
   ],
   "source": [
    "indexes = cv2.dnn.NMSBoxes(boxes, confidences,0.5,0.4 )\n",
    "\n",
    "print(indexes)\n",
    "#As we can see only 5 rectangles are  of use i,e. others are dplicates\n",
    "# Let's add this in the code above itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally the full running Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "# Let's show Info on Screen about the Lables\n",
    "boxes = []\n",
    "confidences = []\n",
    "class_ids = []\n",
    "\n",
    "#Same code again but with rectangle created properly\n",
    "#Showing Info on Screen\n",
    "for out in outs: #outs is basically outputs of all 3 output layers and out is each output layer. \n",
    "    for detection in out: #detection is each bounding box in each layer\n",
    "        scores = detection[5:] #Evet bounding box has 5 values i.e, (x center, y center, width, height, confidence) and probability for each class\n",
    "        class_id = np.argmax(scores) #the index of max score for a class ID is extracted\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > 0.5: #confidence can go from 0-1\n",
    "            #Object is Detected\n",
    "            # Now we're getting Co-ordinates of the object Detected\n",
    "            center_x = int(detection[0] * width) #x coordinate of center\n",
    "            center_y = int(detection[1] * height) #y coordinate of center\n",
    "            w = int(detection[2] * width) #width\n",
    "            h = int(detection[3] * height) #height\n",
    "            \n",
    "            #Rectangle Co-ordinates\n",
    "            x = int(center_x - w / 2) #Top left of x\n",
    "            y = int(center_y - h /2) #Top Left of Y\n",
    "            \n",
    "        \n",
    "            boxes.append([x,y,w,h])\n",
    "            confidences.append(float(confidence))\n",
    "            class_ids.append(class_id)\n",
    "            \n",
    "print(len(boxes)) # 12 objects deteced \n",
    "\n",
    "number_objects_detected = len(boxes)\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "indexes = cv2.dnn.NMSBoxes(boxes, confidences,0.5,0.4 )\n",
    "for i in range(len(boxes)):\n",
    "    if i in indexes:\n",
    "        X,Y,W,H = boxes[i]\n",
    "        label  = str(classes[class_ids[i]])\n",
    "        cv2.rectangle(img, (X,Y),(X + W, Y + H), (0,255,0),2)\n",
    "        cv2.putText(img,label, (X,Y + 30), font, 1, (255,255,255), 1)\n",
    "cv2.imshow(\"Image\",img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
